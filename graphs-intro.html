<!DOCTYPE html>
<html>
    <head>
        <link rel="stylesheet" href="basicstyle.css">
        <meta charset="utf-12">
        <meta name="viewport" content="width=device-width">
        <title>Invisible Map Onboarding Materials | The Model</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>
    <body>
        <a href="index.html">Home</a>
        <h1>The Model: Introduction to Graphs</h1>
        <p>
            Most SLAM solutions represent sensor data as a
            <a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)" target="_blank">graph</a>.
            The main vertices (nodes) that appear in graph-based SLAM are the camera poses and sometimes landmarks,
            and the edges between vertices are constraints (some physical relationship). In the Invisible Map,
            the nodes are camera poses, April tag poses, and dummy nodes (explained further in --SECTION--).
            The edges are the transformations from the camera pose to the tags and dummy nodes at each frame.
            Edges are constraints and thus their values are fixed. SLAM will change the positions of the nodes to
            best fit the edges.
        </p>
        <p>
            Camera, tag and dummy node positions are represented in the world reference frame using a 4-by-4
            transformation matrix. The upper left 3-by-3 matrix represents the rotation and the 4th column
            represents the translation from the origin of the world referene frame. The world reference frame
            has the y axis oriented vertically with gravity and the x and z axis are oriented based on the starting
            position of the phone.
        </p>
        <h2>Camera Nodes</h2>
        <p>
            The camera nodes are the phone odometry data points captured during map creation. InvisibleMapCreator
            captures 10 frames per second, and ARKit tracks the transform of the camera pose. The app then stores
            the information as the user records the map.
        </p>
        <h2>Tag Nodes</h2>
        <p>
            The C++ code in InvisibleMap is used to detect and recognize tags. How the AprilTag detection itself
            works is not necessary to know. The important piece is that the app will save the position of the
            tags as a transform in the world frame. Each separate detection of a tag is saved as a separate
            data point.
        </p>
        <h2>Dummy Nodes</h2>
        <p>
            Dummy nodes are added in the back-end processing before sending the graph through G2O for optimization.
            Their purpose is to restrict the rotation of the camera nodes in order to prevent them from straying
            off the horizontal planes. 
        </p>
    </body>
</html>